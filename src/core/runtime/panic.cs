// Copyright 2014 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.
namespace go;

using abi = @internal.abi_package;
using goarch = @internal.goarch_package;
using atomic = @internal.runtime.atomic_package;
using stringslite = @internal.stringslite_package;
using sys = runtime.@internal.sys_package;
using @unsafe = unsafe_package;
using @internal;
using @internal.runtime;
using runtime.@internal;

partial class runtime_package {

[GoType("num:uint32")] partial struct throwType;

internal static readonly throwType throwTypeNone = /* iota */ 0;
internal static readonly throwType throwTypeUser = 1;
internal static readonly throwType throwTypeRuntime = 2;

// We have two different ways of doing defers. The older way involves creating a
// defer record at the time that a defer statement is executing and adding it to a
// defer chain. This chain is inspected by the deferreturn call at all function
// exits in order to run the appropriate defer calls. A cheaper way (which we call
// open-coded defers) is used for functions in which no defer statements occur in
// loops. In that case, we simply store the defer function/arg information into
// specific stack slots at the point of each defer statement, as well as setting a
// bit in a bitmask. At each function exit, we add inline code to directly make
// the appropriate defer calls based on the bitmask and fn/arg information stored
// on the stack. During panic/Goexit processing, the appropriate defer calls are
// made using extra funcdata info that indicates the exact stack slots that
// contain the bitmask and defer fn/args.

// Check to make sure we can really generate a panic. If the panic
// was generated from the runtime, or from inside malloc, then convert
// to a throw of msg.
// pc should be the program counter of the compiler-generated code that
// triggered this panic.
internal static void panicCheck1(uintptr pc, @string msg) {
    if (goarch.IsWasm == 0 && stringslite.HasPrefix(funcname(findfunc(pc)), "runtime."u8)) {
        // Note: wasm can't tail call, so we can't get the original caller's pc.
        @throw(msg);
    }
    // TODO: is this redundant? How could we be in malloc
    // but not in the runtime? runtime/internal/*, maybe?
    var gp = getg();
    if (gp != nil && (~gp).m != nil && (~(~gp).m).mallocing != 0) {
        @throw(msg);
    }
}

// Same as above, but calling from the runtime is allowed.
//
// Using this function is necessary for any panic that may be
// generated by runtime.sigpanic, since those are always called by the
// runtime.
internal static void panicCheck2(@string err) {
    // panic allocates, so to avoid recursive malloc, turn panics
    // during malloc into throws.
    var gp = getg();
    if (gp != nil && (~gp).m != nil && (~(~gp).m).mallocing != 0) {
        @throw(err);
    }
}

// Many of the following panic entry-points turn into throws when they
// happen in various runtime contexts. These should never happen in
// the runtime, and if they do, they indicate a serious issue and
// should not be caught by user code.
//
// The panic{Index,Slice,divide,shift} functions are called by
// code generated by the compiler for out of bounds index expressions,
// out of bounds slice expressions, division by zero, and shift by negative.
// The panicdivide (again), panicoverflow, panicfloat, and panicmem
// functions are called by the signal handler when a signal occurs
// indicating the respective problem.
//
// Since panic{Index,Slice,shift} are never called directly, and
// since the runtime package should never have an out of bounds slice
// or array reference or negative shift, if we see those functions called from the
// runtime package we turn the panic into a throw. That will dump the
// entire runtime stack for easier debugging.
//
// The entry points called by the signal handler will be called from
// runtime.sigpanic, so we can't disallow calls from the runtime to
// these (they always look like they're called from the runtime).
// Hence, for these, we just check for clearly bad runtime conditions.
//
// The panic{Index,Slice} functions are implemented in assembly and tail call
// to the goPanic{Index,Slice} functions below. This is done so we can use
// a space-minimal register calling convention.

// failures in the comparisons for s[x], 0 <= x < y (y == len(s))
//
//go:yeswritebarrierrec
internal static void goPanicIndex(nint x, nint y) {
    panicCheck1(getcallerpc(), "index out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsIndex));
}

//go:yeswritebarrierrec
internal static void goPanicIndexU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "index out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsIndex));
}

// failures in the comparisons for s[:x], 0 <= x <= y (y == len(s) or cap(s))
//
//go:yeswritebarrierrec
internal static void goPanicSliceAlen(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSliceAlen));
}

//go:yeswritebarrierrec
internal static void goPanicSliceAlenU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSliceAlen));
}

//go:yeswritebarrierrec
internal static void goPanicSliceAcap(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSliceAcap));
}

//go:yeswritebarrierrec
internal static void goPanicSliceAcapU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSliceAcap));
}

// failures in the comparisons for s[x:y], 0 <= x <= y
//
//go:yeswritebarrierrec
internal static void goPanicSliceB(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSliceB));
}

//go:yeswritebarrierrec
internal static void goPanicSliceBU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSliceB));
}

// failures in the comparisons for s[::x], 0 <= x <= y (y == len(s) or cap(s))
internal static void goPanicSlice3Alen(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSlice3Alen));
}

internal static void goPanicSlice3AlenU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSlice3Alen));
}

internal static void goPanicSlice3Acap(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSlice3Acap));
}

internal static void goPanicSlice3AcapU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSlice3Acap));
}

// failures in the comparisons for s[:x:y], 0 <= x <= y
internal static void goPanicSlice3B(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSlice3B));
}

internal static void goPanicSlice3BU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSlice3B));
}

// failures in the comparisons for s[x:y:], 0 <= x <= y
internal static void goPanicSlice3C(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsSlice3C));
}

internal static void goPanicSlice3CU(nuint x, nint y) {
    panicCheck1(getcallerpc(), "slice bounds out of range"u8);
    throw panic(new boundsError(x: ((int64)x), signed: false, y: y, code: boundsSlice3C));
}

// failures in the conversion ([x]T)(s) or (*[x]T)(s), 0 <= x <= y, y == len(s)
internal static void goPanicSliceConvert(nint x, nint y) {
    panicCheck1(getcallerpc(), "slice length too short to convert to array or pointer to array"u8);
    throw panic(new boundsError(x: ((int64)x), signed: true, y: y, code: boundsConvert));
}

// Implemented in assembly, as they take arguments in registers.
// Declared here to mark them as ABIInternal.
internal static partial void panicIndex(nint x, nint y);

internal static partial void panicIndexU(nuint x, nint y);

internal static partial void panicSliceAlen(nint x, nint y);

internal static partial void panicSliceAlenU(nuint x, nint y);

internal static partial void panicSliceAcap(nint x, nint y);

internal static partial void panicSliceAcapU(nuint x, nint y);

internal static partial void panicSliceB(nint x, nint y);

internal static partial void panicSliceBU(nuint x, nint y);

internal static partial void panicSlice3Alen(nint x, nint y);

internal static partial void panicSlice3AlenU(nuint x, nint y);

internal static partial void panicSlice3Acap(nint x, nint y);

internal static partial void panicSlice3AcapU(nuint x, nint y);

internal static partial void panicSlice3B(nint x, nint y);

internal static partial void panicSlice3BU(nuint x, nint y);

internal static partial void panicSlice3C(nint x, nint y);

internal static partial void panicSlice3CU(nuint x, nint y);

internal static partial void panicSliceConvert(nint x, nint y);

internal static error shiftError = ((error)((errorString)"negative shift amount"u8));

//go:yeswritebarrierrec
internal static void panicshift() {
    panicCheck1(getcallerpc(), "negative shift amount"u8);
    throw panic(shiftError);
}

internal static error divideError = ((error)((errorString)"integer divide by zero"u8));

//go:yeswritebarrierrec
internal static void panicdivide() {
    panicCheck2("integer divide by zero"u8);
    throw panic(divideError);
}

internal static error overflowError = ((error)((errorString)"integer overflow"u8));

internal static void panicoverflow() {
    panicCheck2("integer overflow"u8);
    throw panic(overflowError);
}

internal static error floatError = ((error)((errorString)"floating point error"u8));

internal static void panicfloat() {
    panicCheck2("floating point error"u8);
    throw panic(floatError);
}

internal static error memoryError = ((error)((errorString)"invalid memory address or nil pointer dereference"u8));

internal static void panicmem() {
    panicCheck2("invalid memory address or nil pointer dereference"u8);
    throw panic(memoryError);
}

internal static void panicmemAddr(uintptr addr) {
    panicCheck2("invalid memory address or nil pointer dereference"u8);
    throw panic(new errorAddressString(msg: "invalid memory address or nil pointer dereference"u8, addr: addr));
}

// Create a new deferred function fn, which has no arguments and results.
// The compiler turns a defer statement into a call to this.
internal static void deferproc(Action fn) {
    var gp = getg();
    if ((~(~gp).m).curg != gp) {
        // go code on the system stack can't defer
        @throw("defer on system stack"u8);
    }
    var d = newdefer();
    d.val.link = gp.val._defer;
    gp.val._defer = d;
    d.val.fn = fn;
    d.val.pc = getcallerpc();
    // We must not be preempted between calling getcallersp and
    // storing it to d.sp because getcallersp's result is a
    // uintptr stack pointer.
    d.val.sp = getcallersp();
    // deferproc returns 0 normally.
    // a deferred func that stops a panic
    // makes the deferproc return 1.
    // the code the compiler generates always
    // checks the return value and jumps to the
    // end of the function if deferproc returns != 0.
    return0();
}

// No code can go here - the C return register has
// been set and must not be clobbered.
internal static error rangeDoneError = ((error)((errorString)"range function continued iteration after function for loop body returned false"u8));

internal static error rangePanicError = ((error)((errorString)"range function continued iteration after loop body panic"u8));

internal static error rangeExhaustedError = ((error)((errorString)"range function continued iteration after whole loop exit"u8));

internal static error rangeMissingPanicError = ((error)((errorString)"range function recovered a loop body panic and did not resume panicking"u8));

//go:noinline
internal static void panicrangestate(nint state) {
    var exprᴛ1 = ((abi.RF_State)state);
    if (exprᴛ1 == abi.RF_DONE) {
        throw panic(rangeDoneError);
    }
    else if (exprᴛ1 == abi.RF_PANIC) {
        throw panic(rangePanicError);
    }
    else if (exprᴛ1 == abi.RF_EXHAUSTED) {
        throw panic(rangeExhaustedError);
    }
    else if (exprᴛ1 == abi.RF_MISSING_PANIC) {
        throw panic(rangeMissingPanicError);
    }

    @throw("unexpected state passed to panicrangestate"u8);
}

// deferrangefunc is called by functions that are about to
// execute a range-over-function loop in which the loop body
// may execute a defer statement. That defer needs to add to
// the chain for the current function, not the func literal synthesized
// to represent the loop body. To do that, the original function
// calls deferrangefunc to obtain an opaque token representing
// the current frame, and then the loop body uses deferprocat
// instead of deferproc to add to that frame's defer lists.
//
// The token is an 'any' with underlying type *atomic.Pointer[_defer].
// It is the atomically-updated head of a linked list of _defer structs
// representing deferred calls. At the same time, we create a _defer
// struct on the main g._defer list with d.head set to this head pointer.
//
// The g._defer list is now a linked list of deferred calls,
// but an atomic list hanging off:
//
//		g._defer => d4 -> d3 -> drangefunc -> d2 -> d1 -> nil
//	                             | .head
//	                             |
//	                             +--> dY -> dX -> nil
//
// with each -> indicating a d.link pointer, and where drangefunc
// has the d.rangefunc = true bit set.
// Note that the function being ranged over may have added
// its own defers (d4 and d3), so drangefunc need not be at the
// top of the list when deferprocat is used. This is why we pass
// the atomic head explicitly.
//
// To keep misbehaving programs from crashing the runtime,
// deferprocat pushes new defers onto the .head list atomically.
// The fact that it is a separate list from the main goroutine
// defer list means that the main goroutine's defers can still
// be handled non-atomically.
//
// In the diagram, dY and dX are meant to be processed when
// drangefunc would be processed, which is to say the defer order
// should be d4, d3, dY, dX, d2, d1. To make that happen,
// when defer processing reaches a d with rangefunc=true,
// it calls deferconvert to atomically take the extras
// away from d.head and then adds them to the main list.
//
// That is, deferconvert changes this list:
//
//		g._defer => drangefunc -> d2 -> d1 -> nil
//	                 | .head
//	                 |
//	                 +--> dY -> dX -> nil
//
// into this list:
//
//	g._defer => dY -> dX -> d2 -> d1 -> nil
//
// It also poisons *drangefunc.head so that any future
// deferprocat using that head will throw.
// (The atomic head is ordinary garbage collected memory so that
// it's not a problem if user code holds onto it beyond
// the lifetime of drangefunc.)
//
// TODO: We could arrange for the compiler to call into the
// runtime after the loop finishes normally, to do an eager
// deferconvert, which would catch calling the loop body
// and having it defer after the loop is done. If we have a
// more general catch of loop body misuse, though, this
// might not be worth worrying about in addition.
//
// See also ../cmd/compile/internal/rangefunc/rewrite.go.
internal static any deferrangefunc() {
    var gp = getg();
    if ((~(~gp).m).curg != gp) {
        // go code on the system stack can't defer
        @throw("defer on system stack"u8);
    }
    var d = newdefer();
    d.val.link = gp.val._defer;
    gp.val._defer = d;
    d.val.pc = getcallerpc();
    // We must not be preempted between calling getcallersp and
    // storing it to d.sp because getcallersp's result is a
    // uintptr stack pointer.
    d.val.sp = getcallersp();
    d.val.rangefunc = true;
    d.val.head = @new<atomic.Pointer<_defer>>();
    return (~d).head;
}

// badDefer returns a fixed bad defer pointer for poisoning an atomic defer list head.
internal static ж<_defer> badDefer() {
    return (ж<_defer>)(uintptr)(((@unsafe.Pointer)((uintptr)1)));
}

// deferprocat is like deferproc but adds to the atomic list represented by frame.
// See the doc comment for deferrangefunc for details.
internal static void deferprocat(Action fn, any frame) {
    var head = frame._<atomic.Pointer<_defer>.val>();
    if (raceenabled) {
        racewritepc(new @unsafe.Pointer(head), getcallerpc(), abi.FuncPCABIInternal(deferprocat));
    }
    var d1 = newdefer();
    d1.val.fn = fn;
    while (ᐧ) {
        d1.val.link = head.Load();
        if ((~d1).link == badDefer()) {
            @throw("defer after range func returned"u8);
        }
        if (head.CompareAndSwap((~d1).link, d1)) {
            break;
        }
    }
    // Must be last - see deferproc above.
    return0();
}

// deferconvert converts the rangefunc defer list of d0 into an ordinary list
// following d0.
// See the doc comment for deferrangefunc for details.
internal static void deferconvert(ж<_defer> Ꮡd0) {
    ref var d0 = ref Ꮡd0.val;

    var head = d0.head;
    if (raceenabled) {
        racereadpc(new @unsafe.Pointer(head), getcallerpc(), abi.FuncPCABIInternal(deferconvert));
    }
    var tail = d0.link;
    d0.rangefunc = false;
    ж<_defer> d = default!;
    while (ᐧ) {
        d = head.Load();
        if (head.CompareAndSwap(d, badDefer())) {
            break;
        }
    }
    if (d == nil) {
        return;
    }
    for (var d1 = d; ᐧ ; d1 = d1.val.link) {
        d1.val.sp = d0.sp;
        d1.val.pc = d0.pc;
        if ((~d1).link == nil) {
            d1.val.link = tail;
            break;
        }
    }
    d0.link = d;
    return;
}

// deferprocStack queues a new deferred function with a defer record on the stack.
// The defer record must have its fn field initialized.
// All other fields can contain junk.
// Nosplit because of the uninitialized pointer fields on the stack.
//
//go:nosplit
internal static void deferprocStack(ж<_defer> Ꮡd) {
    ref var d = ref Ꮡd.val;

    var gp = getg();
    if ((~(~gp).m).curg != gp) {
        // go code on the system stack can't defer
        @throw("defer on system stack"u8);
    }
    // fn is already set.
    // The other fields are junk on entry to deferprocStack and
    // are initialized here.
    d.heap = false;
    d.rangefunc = false;
    d.sp = getcallersp();
    d.pc = getcallerpc();
    // The lines below implement:
    //   d.panic = nil
    //   d.fd = nil
    //   d.link = gp._defer
    //   d.head = nil
    //   gp._defer = d
    // But without write barriers. The first three are writes to
    // the stack so they don't need a write barrier, and furthermore
    // are to uninitialized memory, so they must not use a write barrier.
    // The fourth write does not require a write barrier because we
    // explicitly mark all the defer structures, so we don't need to
    // keep track of pointers to them with a write barrier.
    ((ж<uintptr>)(uintptr)(((@unsafe.Pointer)(Ꮡ(d.link))))).val = ((uintptr)new @unsafe.Pointer((~gp)._defer));
    ((ж<uintptr>)(uintptr)(((@unsafe.Pointer)(Ꮡ(d.head))))).val = 0;
    ((ж<uintptr>)(uintptr)(((@unsafe.Pointer)(Ꮡ((~gp)._defer))))).val = ((uintptr)new @unsafe.Pointer(Ꮡd));
    return0();
}

// No code can go here - the C return register has
// been set and must not be clobbered.
// Each P holds a pool for defers.

// Allocate a Defer, usually using per-P pool.
// Each defer must be released with freedefer.  The defer is not
// added to any defer chain yet.
internal static ж<_defer> newdefer() {
    ж<_defer> d = default!;
    var mp = acquirem();
    var pp = (~mp).p.ptr();
    if (len((~pp).deferpool) == 0 && sched.deferpool != nil) {
        @lock(Ꮡsched.of(schedt.Ꮡdeferlock));
        while (len((~pp).deferpool) < cap((~pp).deferpool) / 2 && sched.deferpool != nil) {
            var dΔ1 = sched.deferpool;
            sched.deferpool = dΔ1.val.link;
            d.val.link = default!;
            pp.val.deferpool = append((~pp).deferpool, dΔ1);
        }
        unlock(Ꮡsched.of(schedt.Ꮡdeferlock));
    }
    {
        nint n = len((~pp).deferpool); if (n > 0) {
            d = (~pp).deferpool[n - 1];
            (~pp).deferpool[n - 1] = default!;
            pp.val.deferpool = (~pp).deferpool[..(int)(n - 1)];
        }
    }
    releasem(mp);
    (mp, pp) = (default!, default!);
    if (d == nil) {
        // Allocate new defer.
        d = @new<_defer>();
    }
    d.val.heap = true;
    return d;
}

// popDefer pops the head of gp's defer list and frees it.
internal static void popDefer(ж<g> Ꮡgp) {
    ref var gp = ref Ꮡgp.val;

    var d = gp._defer;
    d.val.fn = default!;
    // Can in theory point to the stack
    // We must not copy the stack between the updating gp._defer and setting
    // d.link to nil. Between these two steps, d is not on any defer list, so
    // stack copying won't adjust stack pointers in it (namely, d.link). Hence,
    // if we were to copy the stack, d could then contain a stale pointer.
    gp._defer = d.val.link;
    d.val.link = default!;
    // After this point we can copy the stack.
    if (!(~d).heap) {
        return;
    }
    var mp = acquirem();
    var pp = (~mp).p.ptr();
    if (len((~pp).deferpool) == cap((~pp).deferpool)) {
        // Transfer half of local cache to the central cache.
        ж<_defer> first = default!;
        ж<_defer> last = default!;
        while (len((~pp).deferpool) > cap((~pp).deferpool) / 2) {
            nint n = len((~pp).deferpool);
            var dΔ1 = (~pp).deferpool[n - 1];
            (~pp).deferpool[n - 1] = default!;
            pp.val.deferpool = (~pp).deferpool[..(int)(n - 1)];
            if (first == nil){
                first = dΔ1;
            } else {
                last.val.link = dΔ1;
            }
            last = dΔ1;
        }
        @lock(Ꮡsched.of(schedt.Ꮡdeferlock));
        last.val.link = sched.deferpool;
        sched.deferpool = first;
        unlock(Ꮡsched.of(schedt.Ꮡdeferlock));
    }
    d.val = new _defer(nil);
    pp.val.deferpool = append((~pp).deferpool, d);
    releasem(mp);
    (mp, pp) = (default!, default!);
}

// deferreturn runs deferred functions for the caller's frame.
// The compiler inserts a call to this at the end of any
// function which calls defer.
internal static void deferreturn() {
    _panic Δp = default!;
    Δp.deferreturn = true;
    Δp.start(getcallerpc(), ((@unsafe.Pointer)getcallersp()));
    while (ᐧ) {
        var (fn, ok) = Δp.nextDefer();
        if (!ok) {
            break;
        }
        fn();
    }
}

// Goexit terminates the goroutine that calls it. No other goroutine is affected.
// Goexit runs all deferred calls before terminating the goroutine. Because Goexit
// is not a panic, any recover calls in those deferred functions will return nil.
//
// Calling Goexit from the main goroutine terminates that goroutine
// without func main returning. Since func main has not returned,
// the program continues execution of other goroutines.
// If all other goroutines exit, the program crashes.
public static void Goexit() {
    // Create a panic object for Goexit, so we can recognize when it might be
    // bypassed by a recover().
    _panic Δp = default!;
    Δp.goexit = true;
    Δp.start(getcallerpc(), ((@unsafe.Pointer)getcallersp()));
    while (ᐧ) {
        var (fn, ok) = Δp.nextDefer();
        if (!ok) {
            break;
        }
        fn();
    }
    goexit1();
}

// Call all Error and String methods before freezing the world.
// Used when crashing with panicking.
internal static void preprintpanics(ж<_panic> Ꮡp) => func((defer, _) => {
    ref var Δp = ref Ꮡp.val;

    defer(() => {
        @string text = "panic while printing panic value"u8;
        switch (recover().type()) {
        case default! r: {
            break;
        }
        case @string r: {
            @throw(text + ": "u8 + r);
            break;
        }
        default: {
            var r = recover().type();
            @throw(text + ": type "u8 + toRType((~efaceOf(Ꮡ(r)))._type).@string());
            break;
        }}
    });
    // nothing to do
    while (Δp != nil) {
        switch (Δp.arg.type()) {
        case error v: {
            Δp.arg = v.Error();
            break;
        }
        case stringer v: {
            Δp.arg = v.String();
            break;
        }}
        Δp = Δp.link;
    }
});

// Print all currently active panics. Used when crashing.
// Should only be called after preprintpanics.
internal static void printpanics(ж<_panic> Ꮡp) {
    ref var Δp = ref Ꮡp.val;

    if (Δp.link != nil) {
        printpanics(Δp.link);
        if (!Δp.link.goexit) {
            print("\t");
        }
    }
    if (Δp.goexit) {
        return;
    }
    print("panic: ");
    printpanicval(Δp.arg);
    if (Δp.recovered) {
        print(" [recovered]");
    }
    print("\n");
}

// readvarintUnsafe reads the uint32 in varint format starting at fd, and returns the
// uint32 and a pointer to the byte following the varint.
//
// The implementation is the same with runtime.readvarint, except that this function
// uses unsafe.Pointer for speed.
internal static (uint32, @unsafe.Pointer) readvarintUnsafe(@unsafe.Pointer fd) {
    uint32 r = default!;
    nint shift = default!;
    while (ᐧ) {
        var b = ~(ж<uint8>)(uintptr)(fd);
        fd = (uintptr)add(fd.val, @unsafe.Sizeof(b));
        if (b < 128) {
            return (r + ((uint32)b) << (int)(shift), Ꮡfd);
        }
        r += ((uint32)((uint8)(b & 127))) << (int)(((nint)(shift & 31)));
        shift += 7;
        if (shift > 28) {
            throw panic("Bad varint");
        }
    }
}

// A PanicNilError happens when code calls panic(nil).
//
// Before Go 1.21, programs that called panic(nil) observed recover returning nil.
// Starting in Go 1.21, programs that call panic(nil) observe recover returning a *PanicNilError.
// Programs can change back to the old behavior by setting GODEBUG=panicnil=1.
[GoType] partial struct PanicNilError {
    // This field makes PanicNilError structurally different from
    // any other struct in this package, and the _ makes it different
    // from any struct in other packages too.
    // This avoids any accidental conversions being possible
    // between this struct and some other struct sharing the same fields,
    // like happened in go.dev/issue/56603.
    internal array<ж<PanicNilError>> _ = new(0);
}

[GoRecv] public static @string Error(this ref PanicNilError _) {
    return "panic called with nil argument"u8;
}

[GoRecv] public static void RuntimeError(this ref PanicNilError _) {
}

internal static ж<godebugInc> panicnil = Ꮡ(new godebugInc(name: "panicnil"u8));

// The implementation of the predeclared function panic.
// The compiler emits calls to this function.
//
// gopanic should be an internal detail,
// but widely used packages access it using linkname.
// Notable members of the hall of shame include:
//   - go.undefinedlabs.com/scopeagent
//   - github.com/goplus/igop
//
// Do not remove or change the type signature.
// See go.dev/issue/67401.
//
//go:linkname gopanic
internal static void gopanic(any e) {
    if (e == default!) {
        if (debug.panicnil.Load() != 1){
            e = @new<PanicNilError>();
        } else {
            panicnil.IncNonDefault();
        }
    }
    var gp = getg();
    if ((~(~gp).m).curg != gp) {
        print("panic: ");
        printpanicval(e);
        print("\n");
        @throw("panic on system stack"u8);
    }
    if ((~(~gp).m).mallocing != 0) {
        print("panic: ");
        printpanicval(e);
        print("\n");
        @throw("panic during malloc"u8);
    }
    if ((~(~gp).m).preemptoff != ""u8) {
        print("panic: ");
        printpanicval(e);
        print("\n");
        print("preempt off reason: ");
        print((~(~gp).m).preemptoff);
        print("\n");
        @throw("panic during preemptoff"u8);
    }
    if ((~(~gp).m).locks != 0) {
        print("panic: ");
        printpanicval(e);
        print("\n");
        @throw("panic holding locks"u8);
    }
    ref var p = ref heap(new _panic(), out var Ꮡp);
    Δp.arg = e;
    runningPanicDefers.Add(1);
    Δp.start(getcallerpc(), ((@unsafe.Pointer)getcallersp()));
    while (ᐧ) {
        var (fn, ok) = Δp.nextDefer();
        if (!ok) {
            break;
        }
        fn();
    }
    // If we're tracing, flush the current generation to make the trace more
    // readable.
    //
    // TODO(aktau): Handle a panic from within traceAdvance more gracefully.
    // Currently it would hang. Not handled now because it is very unlikely, and
    // already unrecoverable.
    if (traceEnabled()) {
        traceAdvance(false);
    }
    // ran out of deferred calls - old-school panic now
    // Because it is unsafe to call arbitrary user code after freezing
    // the world, we call preprintpanics to invoke all necessary Error
    // and String methods to prepare the panic strings before startpanic.
    preprintpanics(ᏑΔp);
    fatalpanic(ᏑΔp);
    // should not return
    ((ж<nint>)(uintptr)(default!)).val = 0;
}

// not reached

// start initializes a panic to start unwinding the stack.
//
// If p.goexit is true, then start may return multiple times.
[GoRecv] internal static void start(this ref _panic Δp, uintptr pc, @unsafe.Pointer sp) {
    var gp = getg();
    // Record the caller's PC and SP, so recovery can identify panics
    // that have been recovered. Also, so that if p is from Goexit, we
    // can restart its defer processing loop if a recovered panic tries
    // to jump past it.
    Δp.startPC = getcallerpc();
    Δp.startSP = ((@unsafe.Pointer)getcallersp());
    if (Δp.deferreturn) {
        Δp.sp = sp;
        {
            var s = (ж<savedOpenDeferState>)(uintptr)((~gp).param); if (s != nil) {
                // recovery saved some state for us, so that we can resume
                // calling open-coded defers without unwinding the stack.
                gp.val.param = default!;
                Δp.retpc = s.val.retpc;
                Δp.deferBitsPtr = (ж<byte>)(uintptr)(add(sp.val, (~s).deferBitsOffset));
                Δp.slotsPtr = (uintptr)add(sp.val, (~s).slotsOffset);
            }
        }
        return;
    }
    Δp.link = gp.val._panic;
    gp.val._panic = (ж<_panic>)(uintptr)(noescape((uintptr)@unsafe.Pointer.FromRef(ref Δp)));
    // Initialize state machine, and find the first frame with a defer.
    //
    // Note: We could use startPC and startSP here, but callers will
    // never have defer statements themselves. By starting at their
    // caller instead, we avoid needing to unwind through an extra
    // frame. It also somewhat simplifies the terminating condition for
    // deferreturn.
    (Δp.lr, Δp.fp) = (pc, sp);
    Δp.nextFrame();
}

// nextDefer returns the next deferred function to invoke, if any.
//
// Note: The "ok bool" result is necessary to correctly handle when
// the deferred function itself was nil (e.g., "defer (func())(nil)").
[GoRecv] internal static (Action, bool) nextDefer(this ref _panic Δp) {
    var gp = getg();
    if (!Δp.deferreturn) {
        if ((~gp)._panic != Δp) {
            @throw("bad panic stack"u8);
        }
        if (Δp.recovered) {
            mcall(recovery);
            // does not return
            @throw("recovery failed"u8);
        }
    }
    // The assembler adjusts p.argp in wrapper functions that shouldn't
    // be visible to recover(), so we need to restore it each iteration.
    Δp.argp = (uintptr)add(Δp.startSP, sys.MinFrameSize);
    while (ᐧ) {
        while (Δp.deferBitsPtr != nil) {
            var bits = Δp.deferBitsPtr.val;
            // Check whether any open-coded defers are still pending.
            //
            // Note: We need to check this upfront (rather than after
            // clearing the top bit) because it's possible that Goexit
            // invokes a deferred call, and there were still more pending
            // open-coded defers in the frame; but then the deferred call
            // panic and invoked the remaining defers in the frame, before
            // recovering and restarting the Goexit loop.
            if (bits == 0) {
                Δp.deferBitsPtr = default!;
                break;
            }
            // Find index of top bit set.
            var i = 7 - ((uintptr)sys.LeadingZeros8(bits));
            // Clear bit and store it back.
            bits &= ~(uint8)(1 << (int)(i));
            Δp.deferBitsPtr.val = bits;
            return ((ж<Action>)(uintptr)(add(Δp.slotsPtr, i * goarch.PtrSize)).val, true);
        }
Recheck:
        {
            var d = gp.val._defer; if (d != nil && (~d).sp == ((uintptr)Δp.sp)) {
                if ((~d).rangefunc) {
                    deferconvert(d);
                    popDefer(gp);
                    goto Recheck;
                }
                var fn = d.val.fn;
                // TODO(mdempsky): Instead of having each deferproc call have
                // its own "deferreturn(); return" sequence, we should just make
                // them reuse the one we emit for open-coded defers.
                Δp.retpc = d.val.pc;
                // Unlink and free.
                popDefer(gp);
                return (fn, true);
            }
        }
        if (!Δp.nextFrame()) {
            return (default!, false);
        }
    }
}

// nextFrame finds the next frame that contains deferred calls, if any.
[GoRecv] internal static bool /*ok*/ nextFrame(this ref _panic Δp) {
    bool ok = default!;

    if (Δp.lr == 0) {
        return false;
    }
    var gp = getg();
    systemstack(
    var gpʗ2 = gp;
    () => {
        uintptr limit = default!;
        {
            var d = gpʗ2.val._defer; if (d != nil) {
                limit = d.val.sp;
            }
        }
        ref var u = ref heap(new unwinder(), out var Ꮡu);
        u.initAt(Δp.lr, ((uintptr)Δp.fp), 0, gpʗ2, 0);
        while (ᐧ) {
            if (!u.valid()) {
                Δp.lr = 0;
                return ok;
            }
            if (u.frame.sp == limit) {
                break;
            }
            if (Δp.initOpenCodedDefers(u.frame.fn, ((@unsafe.Pointer)u.frame.varp))) {
                break;
            }
            u.next();
        }
        Δp.lr = u.frame.lr;
        Δp.sp = ((@unsafe.Pointer)u.frame.sp);
        Δp.fp = ((@unsafe.Pointer)u.frame.fp);
        ok = true;
    });
    return ok;
}

[GoRecv] internal static bool initOpenCodedDefers(this ref _panic Δp, ΔfuncInfo fn, @unsafe.Pointer varp) {
    @unsafe.Pointer fd = (uintptr)funcdata(fn, abi.FUNCDATA_OpenCodedDeferInfo);
    if (fd == nil) {
        return false;
    }
    if (fn.deferreturn == 0) {
        @throw("missing deferreturn"u8);
    }
    var (deferBitsOffset, fd) = readvarintUnsafe(fd);
    var deferBitsPtr = (ж<uint8>)(uintptr)(add(varp.val, -((uintptr)deferBitsOffset)));
    if (deferBitsPtr.val == 0) {
        return false;
    }
    // has open-coded defers, but none pending
    var (slotsOffset, fd) = readvarintUnsafe(fd);
    Δp.retpc = fn.entry() + ((uintptr)fn.deferreturn);
    Δp.deferBitsPtr = deferBitsPtr;
    Δp.slotsPtr = (uintptr)add(varp.val, -((uintptr)slotsOffset));
    return true;
}

// The implementation of the predeclared function recover.
// Cannot split the stack because it needs to reliably
// find the stack segment of its caller.
//
// TODO(rsc): Once we commit to CopyStackAlways,
// this doesn't need to be nosplit.
//
//go:nosplit
internal static any gorecover(uintptr argp) {
    // Must be in a function running as part of a deferred call during the panic.
    // Must be called from the topmost function of the call
    // (the function used in the defer statement).
    // p.argp is the argument pointer of that topmost deferred function call.
    // Compare against argp reported by caller.
    // If they match, the caller is the one who can recover.
    var gp = getg();
    var Δp = gp.val._panic;
    if (Δp != nil && !(~Δp).goexit && !(~Δp).recovered && argp == ((uintptr)(~Δp).argp)) {
        Δp.val.recovered = true;
        return (~Δp).arg;
    }
    return default!;
}

//go:linkname sync_throw sync.throw
internal static void sync_throw(@string s) {
    @throw(s);
}

//go:linkname sync_fatal sync.fatal
internal static void sync_fatal(@string s) {
    fatal(s);
}

// throw triggers a fatal error that dumps a stack trace and exits.
//
// throw should be used for runtime-internal fatal errors where Go itself,
// rather than user code, may be at fault for the failure.
//
// NOTE: temporarily marked "go:noinline" pending investigation/fix of
// issue #67274, so as to fix longtest builders.
//
// throw should be an internal detail,
// but widely used packages access it using linkname.
// Notable members of the hall of shame include:
//   - github.com/bytedance/sonic
//   - github.com/cockroachdb/pebble
//   - github.com/dgraph-io/ristretto
//   - github.com/outcaste-io/ristretto
//   - github.com/pingcap/br
//   - gvisor.dev/gvisor
//   - github.com/sagernet/gvisor
//
// Do not remove or change the type signature.
// See go.dev/issue/67401.
//
//go:linkname throw
//go:nosplit
internal static void @throw(@string s) {
    // Everything throw does should be recursively nosplit so it
    // can be called even when it's unsafe to grow the stack.
    systemstack(() => {
        print("fatal error: ");
        printindented(s);
        print("\n");
    });
    fatalthrow(throwTypeRuntime);
}

// fatal triggers a fatal error that dumps a stack trace and exits.
//
// fatal is equivalent to throw, but is used when user code is expected to be
// at fault for the failure, such as racing map writes.
//
// fatal does not include runtime frames, system goroutines, or frame metadata
// (fp, sp, pc) in the stack trace unless GOTRACEBACK=system or higher.
//
//go:nosplit
internal static void fatal(@string s) {
    // Everything fatal does should be recursively nosplit so it
    // can be called even when it's unsafe to grow the stack.
    systemstack(() => {
        print("fatal error: ");
        printindented(s);
        print("\n");
    });
    fatalthrow(throwTypeUser);
}

// runningPanicDefers is non-zero while running deferred functions for panic.
// This is used to try hard to get a panic stack trace out when exiting.
internal static atomic.Uint32 runningPanicDefers;

// panicking is non-zero when crashing the program for an unrecovered panic.
internal static atomic.Uint32 panicking;

// paniclk is held while printing the panic information and stack trace,
// so that two concurrent panics don't overlap their output.
internal static mutex paniclk;

// Unwind the stack after a deferred function calls recover
// after a panic. Then arrange to continue running as though
// the caller of the deferred function returned normally.
//
// However, if unwinding the stack would skip over a Goexit call, we
// return into the Goexit loop instead, so it can continue processing
// defers instead.
internal static void recovery(ж<g> Ꮡgp) {
    ref var gp = ref Ꮡgp.val;

    var Δp = gp._panic;
    var (pc, sp, fp) = (Δp.val.retpc, ((uintptr)(~Δp).sp), ((uintptr)(~Δp).fp));
    var p0 = Δp;
    var saveOpenDeferState = (~Δp).deferBitsPtr != nil && (~Δp).deferBitsPtr.val != 0;
    // Unwind the panic stack.
    for (; Δp != nil && ((uintptr)(~Δp).startSP) < sp; Δp = Δp.val.link) {
        // Don't allow jumping past a pending Goexit.
        // Instead, have its _panic.start() call return again.
        //
        // TODO(mdempsky): In this case, Goexit will resume walking the
        // stack where it left off, which means it will need to rewalk
        // frames that we've already processed.
        //
        // There's a similar issue with nested panics, when the inner
        // panic supercedes the outer panic. Again, we end up needing to
        // walk the same stack frames.
        //
        // These are probably pretty rare occurrences in practice, and
        // they don't seem any worse than the existing logic. But if we
        // move the unwinding state into _panic, we could detect when we
        // run into where the last panic started, and then just pick up
        // where it left off instead.
        //
        // With how subtle defer handling is, this might not actually be
        // worthwhile though.
        if ((~Δp).goexit) {
            (pc, sp) = (Δp.val.startPC, ((uintptr)(~Δp).startSP));
            saveOpenDeferState = false;
            // goexit is unwinding the stack anyway
            break;
        }
        runningPanicDefers.Add(-1);
    }
    gp._panic = Δp;
    if (Δp == nil) {
        // must be done with signal
        gp.sig = 0;
    }
    if (gp.param != nil) {
        @throw("unexpected gp.param"u8);
    }
    if (saveOpenDeferState) {
        // If we're returning to deferreturn and there are more open-coded
        // defers for it to call, save enough state for it to be able to
        // pick up where p0 left off.
        gp.param = new @unsafe.Pointer(Ꮡ(new savedOpenDeferState(
            retpc: (~p0).retpc, // We need to save deferBitsPtr and slotsPtr too, but those are
 // stack pointers. To avoid issues around heap objects pointing
 // to the stack, save them as offsets from SP.

            deferBitsOffset: ((uintptr)new @unsafe.Pointer((~p0).deferBitsPtr)) - ((uintptr)(~p0).sp),
            slotsOffset: ((uintptr)(~p0).slotsPtr) - ((uintptr)(~p0).sp)
        )));
    }
    // TODO(mdempsky): Currently, we rely on frames containing "defer"
    // to end with "CALL deferreturn; RET". This allows deferreturn to
    // finish running any pending defers in the frame.
    //
    // But we should be able to tell whether there are still pending
    // defers here. If there aren't, we can just jump directly to the
    // "RET" instruction. And if there are, we don't need an actual
    // "CALL deferreturn" instruction; we can simulate it with something
    // like:
    //
    //	if usesLR {
    //		lr = pc
    //	} else {
    //		sp -= sizeof(pc)
    //		*(*uintptr)(sp) = pc
    //	}
    //	pc = funcPC(deferreturn)
    //
    // So that we effectively tail call into deferreturn, such that it
    // then returns to the simple "RET" epilogue. That would save the
    // overhead of the "deferreturn" call when there aren't actually any
    // pending defers left, and shrink the TEXT size of compiled
    // binaries. (Admittedly, both of these are modest savings.)
    // Ensure we're recovering within the appropriate stack.
    if (sp != 0 && (sp < gp.stack.lo || gp.stack.hi < sp)) {
        print("recover: ", ((Δhex)sp), " not in [", ((Δhex)gp.stack.lo), ", ", ((Δhex)gp.stack.hi), "]\n");
        @throw("bad recovery"u8);
    }
    // Make the deferproc for this d return again,
    // this time returning 1. The calling function will
    // jump to the standard return epilogue.
    gp.sched.sp = sp;
    gp.sched.pc = pc;
    gp.sched.lr = 0;
    // Restore the bp on platforms that support frame pointers.
    // N.B. It's fine to not set anything for platforms that don't
    // support frame pointers, since nothing consumes them.
    switch (ᐧ) {
    case {} when goarch.IsAmd64 != 0: {
        gp.sched.bp = fp - 2 * goarch.PtrSize;
        break;
    }
    case {} when goarch.IsArm64 != 0: {
        gp.sched.bp = sp - goarch.PtrSize;
        break;
    }}

    // on x86, fp actually points one word higher than the top of
    // the frame since the return address is saved on the stack by
    // the caller
    // on arm64, the architectural bp points one word higher
    // than the sp. fp is totally useless to us here, because it
    // only gets us to the caller's fp.
    gp.sched.ret = 1;
    gogo(Ꮡ(gp.sched));
}

// fatalthrow implements an unrecoverable runtime throw. It freezes the
// system, prints stack traces starting from its caller, and terminates the
// process.
//
//go:nosplit
internal static void fatalthrow(throwType t) {
    var pc = getcallerpc();
    var sp = getcallersp();
    var gp = getg();
    if ((~(~gp).m).throwing == throwTypeNone) {
        (~gp).m.val.throwing = t;
    }
    // Switch to the system stack to avoid any stack growth, which may make
    // things worse if the runtime is in a bad state.
    systemstack(
    var gpʗ2 = gp;
    () => {
        if (isSecureMode()) {
            exit(2);
        }
        startpanic_m();
        if (dopanic_m(gpʗ2, pc, sp)) {
            crash();
        }
        exit(2);
    });
    ((ж<nint>)(uintptr)(default!)).val = 0;
}

// not reached

// fatalpanic implements an unrecoverable panic. It is like fatalthrow, except
// that if msgs != nil, fatalpanic also prints panic messages and decrements
// runningPanicDefers once main is blocked from exiting.
//
//go:nosplit
internal static void fatalpanic(ж<_panic> Ꮡmsgs) {
    ref var msgs = ref Ꮡmsgs.val;

    var pc = getcallerpc();
    var sp = getcallersp();
    var gp = getg();
    bool docrash = default!;
    // Switch to the system stack to avoid any stack growth, which
    // may make things worse if the runtime is in a bad state.
    systemstack(
    var gpʗ2 = gp;
    var runningPanicDefersʗ2 = runningPanicDefers;
    () => {
        if (startpanic_m() && msgs != nil) {
            runningPanicDefersʗ2.Add(-1);
            printpanics(Ꮡmsgs);
        }
        docrash = dopanic_m(gpʗ2, pc, sp);
    });
    if (docrash) {
        // By crashing outside the above systemstack call, debuggers
        // will not be confused when generating a backtrace.
        // Function crash is marked nosplit to avoid stack growth.
        crash();
    }
    systemstack(
    () => {
        exit(2);
    });
    ((ж<nint>)(uintptr)(default!)).val = 0;
}

// not reached

// startpanic_m prepares for an unrecoverable panic.
//
// It returns true if panic messages should be printed, or false if
// the runtime is in bad shape and should just print stacks.
//
// It must not have write barriers even though the write barrier
// explicitly ignores writes once dying > 0. Write barriers still
// assume that g.m.p != nil, and this function may not have P
// in some contexts (e.g. a panic in a signal handler for a signal
// sent to an M with no P).
//
//go:nowritebarrierrec
internal static bool startpanic_m() {
    var gp = getg();
    if (mheap_.cachealloc.size == 0) {
        // very early
        print("runtime: panic before malloc heap initialized\n");
    }
    // Disallow malloc during an unrecoverable panic. A panic
    // could happen in a signal handler, or in a throw, or inside
    // malloc itself. We want to catch if an allocation ever does
    // happen (even if we're not in one of these situations).
    (~(~gp).m).mallocing++;
    // If we're dying because of a bad lock count, set it to a
    // good lock count so we don't recursively panic below.
    if ((~(~gp).m).locks < 0) {
        (~gp).m.val.locks = 1;
    }
    var exprᴛ1 = (~(~gp).m).dying;
    var matchᴛ1 = false;
    if (exprᴛ1 is 0) { matchᴛ1 = true;
        (~gp).m.val.dying = 1;
        panicking.Add(1);
        @lock(Ꮡ(paniclk));
        if (debug.schedtrace > 0 || debug.scheddetail > 0) {
            // Setting dying >0 has the side-effect of disabling this G's writebuf.
            schedtrace(true);
        }
        freezetheworld();
        return true;
    }
    if (exprᴛ1 is 1) { matchᴛ1 = true;
        (~gp).m.val.dying = 2;
        print("panic during panic\n");
        return false;
    }
    if (exprᴛ1 is 2) { matchᴛ1 = true;
        (~gp).m.val.dying = 3;
        print("stack trace unavailable\n");
        exit(4);
        fallthrough = true;
    }
    if (fallthrough || !matchᴛ1) { /* default: */
        exit(5);
        return false;
    }

}

// Something failed while panicking.
// Just print a stack trace and exit.
// This is a genuine bug in the runtime, we couldn't even
// print the stack trace successfully.
// Can't even print! Just exit.
// Need to return something.
internal static bool didothers;

internal static mutex deadlock;

// gp is the crashing g running on this M, but may be a user G, while getg() is
// always g0.
internal static bool dopanic_m(ж<g> Ꮡgp, uintptr pc, uintptr sp) {
    ref var gp = ref Ꮡgp.val;

    if (gp.sig != 0) {
        @string signame = signame(gp.sig);
        if (signame != ""u8){
            print("[signal ", signame);
        } else {
            print("[signal ", ((Δhex)gp.sig));
        }
        print(" code=", ((Δhex)gp.sigcode0), " addr=", ((Δhex)gp.sigcode1), " pc=", ((Δhex)gp.sigpc), "]\n");
    }
    var (level, all, docrash) = gotraceback();
    if (level > 0) {
        if (Ꮡgp != gp.m.curg) {
            all = true;
        }
        if (Ꮡgp != gp.m.g0){
            print("\n");
            goroutineheader(Ꮡgp);
            traceback(pc, sp, 0, Ꮡgp);
        } else 
        if (level >= 2 || gp.m.throwing >= throwTypeRuntime) {
            print("\nruntime stack:\n");
            traceback(pc, sp, 0, Ꮡgp);
        }
        if (!didothers && all) {
            didothers = true;
            tracebackothers(Ꮡgp);
        }
    }
    unlock(Ꮡ(paniclk));
    if (panicking.Add(-1) != 0) {
        // Some other m is panicking too.
        // Let it print what it needs to print.
        // Wait forever without chewing up cpu.
        // It will exit when it's done.
        @lock(Ꮡ(deadlock));
        @lock(Ꮡ(deadlock));
    }
    printDebugLog();
    return docrash;
}

// canpanic returns false if a signal should throw instead of
// panicking.
//
//go:nosplit
internal static bool canpanic() {
    var gp = getg();
    var mp = acquirem();
    // Is it okay for gp to panic instead of crashing the program?
    // Yes, as long as it is running Go code, not runtime code,
    // and not stuck in a system call.
    if (gp != (~mp).curg) {
        releasem(mp);
        return false;
    }
    // N.B. mp.locks != 1 instead of 0 to account for acquirem.
    if ((~mp).locks != 1 || (~mp).mallocing != 0 || (~mp).throwing != throwTypeNone || (~mp).preemptoff != ""u8 || (~mp).dying != 0) {
        releasem(mp);
        return false;
    }
    var status = readgstatus(gp);
    if ((uint32)(status & ~_Gscan) != _Grunning || (~gp).syscallsp != 0) {
        releasem(mp);
        return false;
    }
    if (GOOS == "windows"u8 && (~mp).libcallsp != 0) {
        releasem(mp);
        return false;
    }
    releasem(mp);
    return true;
}

// shouldPushSigpanic reports whether pc should be used as sigpanic's
// return PC (pushing a frame for the call). Otherwise, it should be
// left alone so that LR is used as sigpanic's return PC, effectively
// replacing the top-most frame with sigpanic. This is used by
// preparePanic.
internal static bool shouldPushSigpanic(ж<g> Ꮡgp, uintptr pc, uintptr lr) {
    ref var gp = ref Ꮡgp.val;

    if (pc == 0) {
        // Probably a call to a nil func. The old LR is more
        // useful in the stack trace. Not pushing the frame
        // will make the trace look like a call to sigpanic
        // instead. (Otherwise the trace will end at sigpanic
        // and we won't get to see who faulted.)
        return false;
    }
    // If we don't recognize the PC as code, but we do recognize
    // the link register as code, then this assumes the panic was
    // caused by a call to non-code. In this case, we want to
    // ignore this call to make unwinding show the context.
    //
    // If we running C code, we're not going to recognize pc as a
    // Go function, so just assume it's good. Otherwise, traceback
    // may try to read a stale LR that looks like a Go code
    // pointer and wander into the woods.
    if (gp.m.incgo || findfunc(pc).valid()) {
        // This wasn't a bad call, so use PC as sigpanic's
        // return PC.
        return true;
    }
    if (findfunc(lr).valid()) {
        // This was a bad call, but the LR is good, so use the
        // LR as sigpanic's return PC.
        return false;
    }
    // Neither the PC or LR is good. Hopefully pushing a frame
    // will work.
    return true;
}

// isAbortPC reports whether pc is the program counter at which
// runtime.abort raises a signal.
//
// It is nosplit because it's part of the isgoexception
// implementation.
//
//go:nosplit
internal static bool isAbortPC(uintptr pc) {
    var f = findfunc(pc);
    if (!f.valid()) {
        return false;
    }
    return f.funcID == abi.FuncID_abort;
}

} // end runtime_package
